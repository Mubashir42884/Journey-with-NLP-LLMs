{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad623b5",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "For NLP tasks, we will be using Spacy library.\n",
    "\n",
    "For in-text Notebook installation:\n",
    "> `!pip install spacy`\n",
    "\n",
    "Using CMD  or Anaconda Prompt Command Prompt for installation:\n",
    "> `open cmd > Install with prompts \"conda install -c conda-forge spacy\" (RECOMMENDED!)`\n",
    "\n",
    "Download the language library (en_core_web_sm) for Spacy via CMD/Anaconda:\n",
    "> `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36e70b",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "NLP is and area of CS and AI that is concerned with the interactions between machine and human (natural) languages. Basically, programming the machines to process and analyze human natural language in bigger scale.\n",
    "\n",
    "In general, NLP processing looks like this:\n",
    "> ***Natural Human Language (speech and text in English, French, Bengali) <br> ‚Üí Raw Data (text, speech data) <br> ‚Üí NLP processing (tokenization, normalization, segmentation) <br> ‚Üí Feature Extraction <br> ‚Üí Machine recognizible vector representation (BoW, TF-IDF, Word Embeddings) <br> ‚Üí Machine analysis (ML-DL tasks) <br> ‚Üí NLP Applications (spam detection, chatbots, sentiment analysis etc.)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f31d4d",
   "metadata": {},
   "source": [
    "# NLP Processing Steps\n",
    "\n",
    "There are quite a few steps in the whole processing part of natural language data. But in a traditional best-case scene, we can actually divide it into 9 separate simple steps:\n",
    "\n",
    "#### Step-01: Text Cleaning\n",
    "Removing irrelevant or noisy elements from the text. For example:\n",
    "```\n",
    "\"Hey! üôÇ Check out this link ###https://abc.com!!!\"\n",
    ">> [\"Hey! Check out this link\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-02: Lowercasing\n",
    "Converting all text to lowercase letters for ensuring consisteny over all texts. For example:\n",
    "```\n",
    "\"Hello, Adam Gross! I'm Senat Brown.\"\n",
    ">> [\"hello, Adam Gross! i'm Senat  Brown.\"] (‚úî Preferable way)\n",
    ">> [\"hello, adam gross! i'm senat  brown.\"] (‚ùå Not Preferable as it looses Named Entity Recognition)\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-03: Sentence Segmentation\n",
    "Splitting different lines and sentences into separate entity. For example:\n",
    "```\n",
    "\"Sally is mumbling. She might be nervous of speaking.\"\n",
    ">> [\"Sally is mumbling.\", \"She might be nervous of speaking.\"]\n",
    "```\n",
    "‚òÖ Useful for parsing and document-level analysis\n",
    "<br><br>\n",
    "\n",
    "#### Step-04: Tokenization\n",
    "Breaking sentences into preferable peices (Tokens) of words and symbols. For example:\n",
    "```\n",
    "\"I'm Gary Hunson. A DYI shop owner at Brisbey.\"\n",
    ">> [\"I'm\", \"Gary\", \"Hunson\", \".\", \"A\", \"DYI\", \"shop\", \"owner\", \"at\", \"Brisbey\", \".\"]\n",
    "```\n",
    "‚òÖ Tokenization depends on the task in-hand and the context we're working with. It will be clear in upcoming notebooks.\n",
    "<br><br>\n",
    "\n",
    "#### Step-05: Normalization\n",
    "Standardizing text formats to ensure consistency over all type of texts. Oftenly results in better accuracy in classification related tasks. This includes:\n",
    "- Expanding Contraction (don't ‚Üí do not)\n",
    "- Removing extra Punctuation (Hello!! ‚Üí Hello!)\n",
    "- Converting Numbers to text [optional] (3 ‚Üí three)\n",
    "\n",
    "For example:\n",
    "```\n",
    "\"I can't do 9 to 5 anymore!!!\"\n",
    ">> [\"I cannot do nine to five anymore!\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-06: Stopword Removal\n",
    "It is the process of eliminating very common words (am, was, is, to, a, an) that carry less standalone meaning in a sentence and often do not help a model distinguish between texts. For example:\n",
    "```\n",
    "\"I am learning representation learning techniques.\"\n",
    ">> [\"learning\", \"representation\", \"learning\", \"techniques\"]\n",
    "```\n",
    "‚òÖ Removing stopwords:\n",
    "- Reduces noise in text\n",
    "- Reduces vocabulary size\n",
    "- Improves model efficieny\n",
    "- Focuses on content-bearing words\n",
    "<br>\n",
    "\n",
    "#### Step-07: Stemming or Lemmatization\n",
    "**Stemming** is not always a good choice, but faster processing technique.\n",
    "> `studying ‚Üí studi` | Faster processing but not a proper stem<br>\n",
    "> `running ‚Üí run` | A proper stem\n",
    "\n",
    "On the other hand, **Lemmatization** is preferable and accurate in practice, but slower in processing.\n",
    "> `studying ‚Üí study` | A proper lemma <br>\n",
    "> `running ‚Üí run` | A proper lemma\n",
    "<br>\n",
    "\n",
    "#### Step-08: Handling Rare Words and Noise\n",
    "Removing very unknown words and replaceing it with `<UNK>` token is often used for better processing. For example:\n",
    "\n",
    "```\n",
    "\"qwerty keyboard\"\n",
    ">> [<UNK>, \"keyboard\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-09: Vectorization\n",
    "This is a crucial and final process of converting the tokenized texts into numerical vector representation that machine can understand. This is done by various techniques, like:\n",
    "1. **Bag of Words (BoW)** = Represents text by counting the number of times each word appears and ignores the word order.\n",
    "```\n",
    "Text = \"learning representation learning techniques\"\n",
    "Tokens = [\"learning\", \"representation\", \"learning\", \"techniques\"]\n",
    "Vector = [2, 1, 1] (learning 2x, representation 1x, techniques 1x)\n",
    "```\n",
    "2. **TF-IDF (Term Frequency‚ÄìInverse Document Frequency)** = It improves BoW by reducing the importance of very common words (is, was, am) and highlighting distinctive words.<br>\n",
    "TF: how often a word appears in a document<br>\n",
    "IDF: how rare the word is across documents\n",
    "```\n",
    "learning ‚Üí 0.32,        representation ‚Üí 0.78,        techniques ‚Üí 0.64\n",
    "Vector = [0.32, 0.78, 0.64]\n",
    "```\n",
    "3. **Word Embeddings** = Word embeddings represent each word as a dense numerical vector that captures semantic meaning. Words with similar meanings have similar vectors.<br>\n",
    "```\n",
    "learning        ‚Üí [0.21, -0.34, 0.87, ...]\n",
    "representation  ‚Üí [0.19, -0.30, 0.82, ...]\n",
    "techniques     ‚Üí [0.25, -0.40, 0.90, ...]\n",
    "```\n",
    "- Key Learning: **representation learning ‚âà feature learning** (the word \"*feature*\" would have quite similar vector like \"*representation*\"\n",
    "\n",
    "4. **Token IDs** = This technique mostly used by the modern DL (LSTM, BERT, GPT) Transformer models, uses integer token IDs produced by tokenizer.<br>\n",
    "```\n",
    "learning       ‚Üí 1045\n",
    "representation ‚Üí 2381\n",
    "techniques     ‚Üí 7129\n",
    "Token          = [\"learning\", \"representation\", \"learning\", \"techniques\"]\n",
    "Token IDs      = [1045, 2381, 1045, 7129]\n",
    "```\n",
    "- **Important Note**\n",
    "    - Stopword removal is often skipped for transformers\n",
    "    - Tokenizers handle casing, subwords, and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95ecef",
   "metadata": {},
   "source": [
    "## Spacy Basics\n",
    "- Loading library\n",
    "- Building a pipeline object\n",
    "- Using tokens\n",
    "- Parts-of-Speech Tagging\n",
    "- Understanding token attiributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588eb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads language processing model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0b41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple text is passed through the `nlp()` object for NLP processing\n",
    "doc = nlp(u'Hartell is looking for a company with $500m asset to start business in the U.A.E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3020445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartell\n",
      "is\n",
      "looking\n",
      "for\n",
      "a\n",
      "company\n",
      "with\n",
      "$\n",
      "500\n",
      "m\n",
      "asset\n",
      "to\n",
      "start\n",
      "business\n",
      "in\n",
      "the\n",
      "U.A.E\n"
     ]
    }
   ],
   "source": [
    "# To show each generated tokens from the text\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a4ee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartell 96\n",
      "is 87\n",
      "looking 100\n",
      "for 85\n",
      "a 90\n",
      "company 92\n",
      "with 85\n",
      "$ 99\n",
      "500 93\n",
      "m 92\n",
      "asset 92\n",
      "to 94\n",
      "start 100\n",
      "business 92\n",
      "in 85\n",
      "the 90\n",
      "U.A.E 96\n"
     ]
    }
   ],
   "source": [
    "# To show the POS of each generated tokens from the text (as token IDs)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0ec9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartell = PROPN\n",
      "is = AUX\n",
      "looking = VERB\n",
      "for = ADP\n",
      "a = DET\n",
      "company = NOUN\n",
      "with = ADP\n",
      "$ = SYM\n",
      "500 = NUM\n",
      "m = NOUN\n",
      "asset = NOUN\n",
      "to = PART\n",
      "start = VERB\n",
      "business = NOUN\n",
      "in = ADP\n",
      "the = DET\n",
      "U.A.E = PROPN\n"
     ]
    }
   ],
   "source": [
    "# for detailed corresponding POS view\n",
    "for token in doc:\n",
    "    print(token.text, '=', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d1aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartell PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "for ADP prep\n",
      "a DET det\n",
      "company NOUN pobj\n",
      "with ADP prep\n",
      "$ SYM nmod\n",
      "500 NUM nummod\n",
      "m NOUN quantmod\n",
      "asset NOUN pobj\n",
      "to PART aux\n",
      "start VERB advcl\n",
      "business NOUN dobj\n",
      "in ADP prep\n",
      "the DET det\n",
      "U.A.E PROPN pobj\n"
     ]
    }
   ],
   "source": [
    "# for showing syntactic dependency\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad37435b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x2d4c4abd100>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2d4c4abd880>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2d4c4ab6120>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2d4c4b63d80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x2d4c4b75640>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2d4c4ab62e0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999334a",
   "metadata": {},
   "source": [
    "The `nlp.pipeline` is an *ordered list* of components that a text passes through for processing. When you call an nlp object on a text, the text is first tokenized, and then each component in the pipeline is applied to the `Doc` object in sequence, modifying it and adding annotations. \n",
    "\n",
    "`Text ‚Üí nlp.pipeline() ‚Üí [Tokenizer ‚Üí POS Tagger ‚Üí Dependency Parser ‚Üí NER]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aee9c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Hartell's aren't    looking for a company anymore. Are they? Almost...around 1.5K applied and 200 got in 2025!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "501918ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token       POS      Dependency     Description\n",
      "------------------------------------------------------------\n",
      "Hartell    PROPN      nsubj        nominal subject\n",
      "'s         PART       case         case marking\n",
      "are        AUX        aux          auxiliary\n",
      "n't        PART       neg          negation modifier\n",
      "           SPACE      dep          unclassified dependent\n",
      "looking    VERB       ROOT         root\n",
      "for        ADP        prep         prepositional modifier\n",
      "a          DET        det          determiner\n",
      "company    NOUN       pobj         object of preposition\n",
      "anymore    ADV        advmod       adverbial modifier\n",
      ".          PUNCT      punct        punctuation\n",
      "Are        AUX        ROOT         root\n",
      "they       PRON       nsubj        nominal subject\n",
      "?          PUNCT      punct        punctuation\n",
      "Almost     ADV        advmod       adverbial modifier\n",
      "...        PUNCT      punct        punctuation\n",
      "around     ADP        advmod       adverbial modifier\n",
      "1.5        NUM        nummod       numeric modifier\n",
      "K          NOUN       nsubj        nominal subject\n",
      "applied    VERB       ROOT         root\n",
      "and        CCONJ      cc           coordinating conjunction\n",
      "200        NUM        nsubj        nominal subject\n",
      "got        VERB       conj         conjunct\n",
      "in         ADP        prep         prepositional modifier\n",
      "2025       NUM        pobj         object of preposition\n",
      "!          PUNCT      punct        punctuation\n"
     ]
    }
   ],
   "source": [
    "# to show the pos tag, dependency of each token\n",
    "print(f\"Token{'':<7}POS{'':<6}Dependency{'':<5}Description\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for token in doc2:\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.dep_:<12} {spacy.explain(token.dep_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b3df6948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hartell"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0] #show the tokenized word at index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6fe08491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looking'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[5].text # similar but as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f510e07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[5].pos_ # show the POS of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8a4083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROOT'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[5].dep_ # show the dependencies of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e3fd239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'look'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[5].lemma_ # show the Base form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dcdef6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VBG'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[5].tag_ # show the detailed POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ecf17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xxxxx'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].shape_ # the word's shape - caps, puctuation, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ef18591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].is_alpha # if the token contains only alphabetic characters or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2b2ccf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3be15b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[17].is_alpha # 1.5 is numeric, not alphabetic; so it's False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "832a4520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7ca8d3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[7].is_stop # if the token is a part of a stopword or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fb379922",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(\n",
    "    u\"On this I took comfort in spite of all my sorrow, and said, ‚ÄòI know, then, about these two; tell me, therefore, about the third man of whom you spoke; is he still alive, but at sea, and unable to get home? or is he dead? Tell me, no matter how much it may grieve me.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65dd4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = doc3[15:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d72ae111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÄòI know, then, about these two;\n"
     ]
    }
   ],
   "source": [
    "print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45616e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98e4de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25906d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "674b8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc4.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8168c2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6] #7th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f609b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6].is_sent_start # to check if the token is the first token in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "451e78d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe5168f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[7].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a40fbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6].is_sent_end # to check if the token is the last token in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "352e6e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3da6dcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[5].is_sent_end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP GPU",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
