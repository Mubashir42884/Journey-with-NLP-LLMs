{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0f97dd",
   "metadata": {},
   "source": [
    "# NLP Processing - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722a1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d54953",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9a9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're in a hurry for work in U.K.!\n"
     ]
    }
   ],
   "source": [
    "mystring = \"You\\'re in a hurry for work in U.K.!\"\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e80326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n",
      "'re\n",
      "in\n",
      "a\n",
      "hurry\n",
      "for\n",
      "work\n",
      "in\n",
      "U.K.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(mystring)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c758a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's have a user-based experience on the 1st demo of our site https://www.rhsp.com!\n"
     ]
    }
   ],
   "source": [
    "mystring2 = \"Let's have a user-based experience on the 1st demo of our site https://www.rhsp.com!\"\n",
    "print(mystring2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d16097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let | 's | have | a | user | - | based | experience | on | the | 1st | demo | of | our | site | https://www.rhsp.com | ! | "
     ]
    }
   ],
   "source": [
    "doc = nlp(mystring2)\n",
    "for token in doc:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e478b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achilles confronts Paris about what Hector died for, Troy or honor?\n"
     ]
    }
   ],
   "source": [
    "mystring3 = \"Achilles confronts Paris about what Hector died for, Troy or honor?\"\n",
    "print(mystring3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b685d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris \t= Countries, cities, states\n",
      "Hector \t= People, including fictional\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(mystring3)\n",
    "# for named entities\n",
    "for entity in doc1.ents:\n",
    "    print(entity, \"\\t=\", spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e532225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google               = ORG = Companies, agencies, institutions, etc.\n",
      "$300 million dollar  = MONEY = Monetary values, including unit\n",
      "Taiwan               = GPE = Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"Google is not investing $300 million dollar for Taiwan-based stratups\")\n",
    "\n",
    "for entity in doc2.ents:\n",
    "    print(f\"{entity.text:<20} = {entity.label_} = {spacy.explain(entity.label_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2083d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achilles confronts\n",
      "what\n",
      "Hector\n",
      "Troy\n",
      "honor\n"
     ]
    }
   ],
   "source": [
    "# noun chunks\n",
    "for chunk in doc1.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597ec3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n",
      "$300 million dollar\n",
      "Taiwan-based stratups\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc2.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664392d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP GPU",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
