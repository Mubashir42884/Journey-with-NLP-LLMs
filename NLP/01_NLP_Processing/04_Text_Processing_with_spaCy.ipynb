{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eac75af",
   "metadata": {},
   "source": [
    "# Text Processing - Stemming, Lemmatization, Stopwords, Phrase Matching & Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941a2a0",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stripping-off of suffix from long words to make shorter base non-words. It is faster and easier to process but potentially creates non-word which adds less meaning in text processing. For example:\n",
    "<br>\n",
    "\n",
    "`running → run` <br>\n",
    "`studies → studi` <br>\n",
    "\n",
    "<p style=\"margin-left: 60px;\">\n",
    "    <span style=\"color:salmon;\"> For the crude method of Stemming to strip words, <b>spacy</b> doesn't include any stemmer method. Instead, they use Lemmatization, which we will learn later in this notebook.</span><br>\n",
    "</p>\n",
    "\n",
    "So for now, we are going to use `NLTK` package for the stemming process of text.\n",
    "\n",
    "#### Type of Stemmers\n",
    "1. **Porter Stemmer (Porter's Algorithm):** Uses 5 phases of word reduction with different mapping rules-<br>\n",
    "    ***Mapping Rules:***\n",
    "    | word → stem | Example |\n",
    "    | --- | --- |\n",
    "    | SSES → SS | Actresses → Actress |\n",
    "    | IES → I | Pastries → Pastri |\n",
    "    | SS → SS | Dress → Dress |\n",
    "    | S →  | Dogs → Dog |\n",
    "    | (m>0)ATIONAL → ATE | Relational → Relate<br>National → National |\n",
    "    | (m>0)EED → EE | Agreed → Agree<br>Deed → Deed |\n",
    "    \n",
    " <br>\n",
    "2. **Snowball Stemmer (aka Porter2 Algorithm):** Revised and more accurate improvement of the previous, known as \"English Stemmer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f708e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9062b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Porter's Stemming Algorithm:\n",
      " dream           → dream\n",
      " dreamer         → dreamer\n",
      " dreaming        → dream\n",
      " dreams          → dream\n",
      " study           → studi\n",
      " studying        → studi\n",
      " studies         → studi\n",
      " studious        → studiou\n",
      " love            → love\n",
      " lover           → lover\n",
      " loves           → love\n",
      " lovely          → love\n",
      " loving          → love\n",
      " sensational     → sensat\n",
      " relational      → relat\n",
      " disagreed       → disagre\n",
      " parties         → parti\n",
      " poorness        → poor\n",
      " poverty         → poverti\n",
      " poorly          → poorli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "words = ['dream', 'dreamer', 'dreaming', 'dreams',\n",
    "         'study', 'studying', 'studies', 'studious',\n",
    "         'love', 'lover', 'loves', 'lovely', 'loving',\n",
    "         'sensational', 'relational', 'disagreed', 'parties', \n",
    "         'poorness', 'poverty', 'poorly']\n",
    "\n",
    "print(\"Using Porter's Stemming Algorithm:\")\n",
    "for word in words:\n",
    "    print(f\" {word:<15} → {p_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ba979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Snowball Stemming Algorithm:\n",
      " dream           → dream\n",
      " dreamer         → dreamer\n",
      " dreaming        → dream\n",
      " dreams          → dream\n",
      " study           → studi\n",
      " studying        → studi\n",
      " studies         → studi\n",
      " studious        → studious\n",
      " love            → love\n",
      " lover           → lover\n",
      " loves           → love\n",
      " lovely          → love\n",
      " loving          → love\n",
      " sensational     → sensat\n",
      " relational      → relat\n",
      " disagreed       → disagre\n",
      " parties         → parti\n",
      " poorness        → poor\n",
      " poverty         → poverti\n",
      " poorly          → poor\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "words = ['dream', 'dreamer', 'dreaming', 'dreams',\n",
    "         'study', 'studying', 'studies', 'studious',\n",
    "         'love', 'lover', 'loves', 'lovely', 'loving',\n",
    "         'sensational', 'relational', 'disagreed', 'parties', \n",
    "         'poorness', 'poverty', 'poorly']\n",
    "\n",
    "print(\"Using Snowball Stemming Algorithm:\")\n",
    "for word in words:\n",
    "    print(f\" {word:<15} → {s_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418a35a",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike stemming, lemmatization is the process of reducing longer words to the correct base word (lemma) based on linguistic knowledge and correct parts of speech context. It is better and accurate than stemming, but slower and morphological in terms of language. For example:<br>\n",
    "`studying → study`<br>\n",
    "`mice → mouse`<br>\n",
    "`am, is, are, was, were, been, being → be`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a67ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e006a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = (f\"I am a lover who loves to read lovely poems with a loving partner\"\n",
    "f\" who lovingly partners with me. Last time I broke my previous record of reading\"\n",
    "f\" a book while my nose was running, breaking it with a studious boy who was\" \n",
    "f\" studying about Physics. I saw couple of mouse squeeking in the back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494bad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON   4690420944186131903      I\n",
      "am          AUX    10382539506755952630     be\n",
      "a           DET    11901859001352538922     a\n",
      "lover       NOUN   2827967690759983985      lover\n",
      "who         PRON   3876862883474502309      who\n",
      "loves       VERB   3702023516439754181      love\n",
      "to          PART   3791531372978436496      to\n",
      "read        VERB   11792590063656742891     read\n",
      "lovely      ADJ    12747335289542760454     lovely\n",
      "poems       NOUN   3429842728468313404      poem\n",
      "with        ADP    12510949447758279278     with\n",
      "a           DET    11901859001352538922     a\n",
      "loving      VERB   3702023516439754181      love\n",
      "partner     NOUN   15215732398709813157     partner\n",
      "who         PRON   3876862883474502309      who\n",
      "lovingly    ADV    10008751644619825950     lovingly\n",
      "partners    VERB   15215732398709813157     partner\n",
      "with        ADP    12510949447758279278     with\n",
      "me          PRON   4690420944186131903      I\n",
      ".           PUNCT  12646065887601541794     .\n",
      "Last        ADJ    10321518907502812892     last\n",
      "time        NOUN   8885804376230376864      time\n",
      "I           PRON   4690420944186131903      I\n",
      "broke       VERB   5527797886271786622      break\n",
      "my          PRON   227504873216781231       my\n",
      "previous    ADJ    12156979554960747529     previous\n",
      "record      NOUN   12677120423429974351     record\n",
      "of          ADP    886050111519832510       of\n",
      "reading     VERB   11792590063656742891     read\n",
      "a           DET    11901859001352538922     a\n",
      "book        NOUN   13814433107111459297     book\n",
      "while       SCONJ  1039541750886098100      while\n",
      "my          PRON   227504873216781231       my\n",
      "nose        NOUN   18134474571659801292     nose\n",
      "was         AUX    10382539506755952630     be\n",
      "running     VERB   12767647472892411841     run\n",
      ",           PUNCT  2593208677638477497      ,\n",
      "breaking    VERB   5527797886271786622      break\n",
      "it          PRON   10239237003504588839     it\n",
      "with        ADP    12510949447758279278     with\n",
      "a           DET    11901859001352538922     a\n",
      "studious    ADJ    18244412806944160880     studious\n",
      "boy         NOUN   6943052338024606995      boy\n",
      "who         PRON   3876862883474502309      who\n",
      "was         AUX    10382539506755952630     be\n",
      "studying    VERB   4251533498015236010      study\n",
      "about       ADP    942632335873952620       about\n",
      "Physics     PROPN  5247849196256290833      Physics\n",
      ".           PUNCT  12646065887601541794     .\n",
      "I           PRON   4690420944186131903      I\n",
      "saw         VERB   11925638236994514241     see\n",
      "couple      NOUN   16116837014497452873     couple\n",
      "of          ADP    886050111519832510       of\n",
      "mouse       NOUN   1384165645700560590      mouse\n",
      "squeeking   NOUN   1111503358337834227      squeeking\n",
      "in          ADP    3002984154512732771      in\n",
      "the         DET    7425985699627899538      the\n",
      "back        NOUN   15255859468896132977     back\n",
      ".           PUNCT  12646065887601541794     .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text1)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12}{token.pos_:<7}{token.lemma:<25}{token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b43bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            → I\n",
      "am           → be\n",
      "a            → a\n",
      "lover        → lover\n",
      "who          → who\n",
      "loves        → love\n",
      "to           → to\n",
      "read         → read\n",
      "lovely       → lovely\n",
      "poems        → poem\n",
      "with         → with\n",
      "a            → a\n",
      "loving       → love\n",
      "partner      → partner\n",
      "who          → who\n",
      "lovingly     → lovingly\n",
      "partners     → partner\n",
      "with         → with\n",
      "me           → I\n",
      ".            → .\n",
      "Last         → last\n",
      "time         → time\n",
      "I            → I\n",
      "broke        → break\n",
      "my           → my\n",
      "previous     → previous\n",
      "record       → record\n",
      "of           → of\n",
      "reading      → read\n",
      "a            → a\n",
      "book         → book\n",
      "while        → while\n",
      "my           → my\n",
      "nose         → nose\n",
      "was          → be\n",
      "running      → run\n",
      ",            → ,\n",
      "breaking     → break\n",
      "it           → it\n",
      "with         → with\n",
      "a            → a\n",
      "studious     → studious\n",
      "boy          → boy\n",
      "who          → who\n",
      "was          → be\n",
      "studying     → study\n",
      "about        → about\n",
      "Physics      → Physics\n",
      ".            → .\n",
      "I            → I\n",
      "saw          → see\n",
      "couple       → couple\n",
      "of           → of\n",
      "mouse        → mouse\n",
      "squeeking    → squeeking\n",
      "in           → in\n",
      "the          → the\n",
      "back         → back\n",
      ".            → .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:<12} → {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17d4ae",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Stopwords are the most common and most used words that doesn't add up to any meaning to a text. Like: <br>\n",
    "`a, an, the, this, in, and, is, of, on, to, for, with, that,..., etc.`<br>\n",
    "Almost 326 built-in list of stopwords are in spacy library. These words can be easily filtered from the text to process with the actual, meaningful text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5115bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'third', 're', 'other', 'few', 'own', 'next', 'eleven', 'will', 'to', 'whether', 'afterwards', 'of', 'six', 'again', 'however', 'upon', 'too', 'least', 'herself', 'not', 'unless', 'would', \"'ve\", 'may', 'two', '’m', 'together', 'within', 'anyway', 'be', 'their', 'therefore', 'against', 'then', 'ourselves', 'regarding', 'that', 'since', 'anywhere', 'put', 'serious', 'by', 'do', 'n’t', 'how', 'so', 'everything', 'four', 'had', 'nine', 'namely', '’re', 'for', 'in', 'must', 'yourselves', 'beforehand', '‘re', 'everywhere', 'until', 'whereas', 'any', 'thereafter', 'yours', 'yet', 'more', 'did', 'behind', 'and', 'was', 'just', 'or', 'get', 'thereupon', 'what', 'therein', 'per', 'about', 'moreover', 'his', 'because', 'ever', 'themselves', 'only', 'should', 'he', 'besides', 'became', 'nowhere', 'anyone', 'sometimes', 'enough', 'both', 'mostly', \"n't\", 'she', 'quite', 'before', 'across', 'as', 'empty', 'thence', 'seem', 'which', 'perhaps', 'doing', 'none', 'take', 'might', 'been', 'into', 'everyone', 'when', 'neither', 'thru', 'elsewhere', 'please', 'my', 'we', 'nobody', '‘d', 'many', 'beyond', 'others', 'between', 'are', 'whole', 'noone', 'top', 'while', 'almost', 'latter', 'well', 'wherein', 'all', 'us', 'former', 'from', 'at', \"'d\", 'less', 'nevertheless', 'meanwhile', 'throughout', 'though', 'part', \"'s\", 'somehow', 'become', 'does', 'some', 'fifteen', 'always', 'such', 'were', 'although', 'hers', 'an', 'am', 'somewhere', '’d', 'along', 'here', 'whereby', 'mine', '’ve', 'back', 'hundred', 'onto', 'say', 'side', 'whence', 'me', 'with', 'n‘t', 'ten', 'front', 'really', 'made', 'another', 'yourself', 'further', \"'m\", 'already', 'often', 'can', 'otherwise', 'much', 'hereby', 'else', 'name', 'still', 'someone', 'him', 'twenty', 'becoming', 'on', 'whoever', 'twelve', 'last', 'without', 'herein', 'nothing', 'our', 'anyhow', 'has', 'hereupon', 'you', 'beside', 'forty', 'see', '‘ll', 'after', \"'ll\", 'it', 'this', 'make', 'move', 'something', 'seeming', 'up', 'seems', 'over', 'nor', 'show', '‘s', 'below', 'down', 'whither', '’ll', 'indeed', 'off', 'thereby', 'those', 'rather', 'but', 'seemed', 'amount', 'these', 'give', 'one', 'whenever', 'five', 'is', 'either', 'using', 'there', 'have', 'whatever', 'why', 'whom', 'myself', 'its', 'becomes', 'no', 'except', 'your', 'her', 'fifty', 'most', 'even', 'toward', 'hence', 'via', 'bottom', 'cannot', 'every', 'used', 'also', 'anything', 'eight', 'than', 'ca', 'due', 'a', \"'re\", 'himself', 'full', 'several', 'the', 'out', 'now', 'call', 'go', 'if', 'i', 'towards', 'alone', 'once', 'among', 'same', 'whereafter', 'they', 'each', 'latterly', '’s', 'three', 'first', '‘ve', 'wherever', 'done', 'who', 'them', 'itself', 'very', 'above', 'sometime', 'thus', 'under', 'never', 'various', 'being', 'could', '‘m', 'through', 'amongst', 'keep', 'formerly', 'during', 'whose', 'ours', 'sixty', 'around', 'hereafter', 'whereupon', 'where'}\n"
     ]
    }
   ],
   "source": [
    "# list of all stopwords in the 'en_core_web_sm' model of spicy\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4556072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of total stopwords\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b03334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['The'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2f9bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a82d6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To add a new stopwords to the list for yourself\"\"\"\n",
    "\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbc70a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To remove a new stopword from the list for yourself\"\"\"\n",
    "\n",
    "nlp.Defaults.stop_words.remove('btw')\n",
    "nlp.vocab['btw'].is_stop = False\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85d807",
   "metadata": {},
   "source": [
    "### Phrase Matching & Vocabularies\n",
    "The process of identifying and labeling specific phrases that match pattern defined by ourselves. This can be looked as a powerful and better version of Regular Expression where we can actually take parts of speech to search for our patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1b2e4",
   "metadata": {},
   "source": [
    "#### List of Token Attributes\n",
    "\n",
    "| Attributes | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| `ORTH`| The exact verbatim text of a token| `ORTH(\"ApPlE\")` → `\"ApPlE\"` |\n",
    "| `LOWER` | The lowercase form of the token text | `LOWER(\"Happy\")` → `\"happy\"`|\n",
    "| `LENGTH` | The length of the token text | `LENGTH(\"5:7\")` → `\"Jello\", \"Rabbit\", \"Pokemon\"` |\n",
    "| `IS_ALPHA, IS_ASCII, IS_DIGIT` | Token consists of alphanumerics, ASCII, and digits | `IS_ALPHA(\"HOMER\")` → `True`,<br>`IS_DIGIT(35)` → `True` |\n",
    "| `IS_LOWER, IS_UPPER, IS_TITLE` | Token text in lowercase, uppercase, and titlecase | `\"parsimony\", \"CRUEL\", \"SpaCy\"`|\n",
    "| `IS_PUNCT, IS_SPACE, IS_STOP` | Token text is punctuation, whitespace, and stopword | `\"!\", \" \", \"the\"` |\n",
    "| `LIKE_NUM, LIKE_URL, LIKE_EMAIL` | Token text resembles a number, URL, email | `\"902-768-123\", \"https://\", \"@yahoo.com\"`|\n",
    "| `POS, TAG, DEP, LEMMA, SHAPE` | Token's POS, tag, dependency label, lemma, shape | `\"VERB\", \"NNP\", \"dobj\", \"xxxx\"` |\n",
    "| `ENT_TYPE` | The token's entity label| `\"GEP\", \"ORG\", \"MONEY\"` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b40cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198c0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90be467f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" To detect:\n",
    "        + solarpower  = transform to all lowercase\n",
    "        + Solar-power = check for punctuation in between\n",
    "        + Solar power = as 2 seperate words\n",
    "\"\"\"\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "pattern2 = [{'LOWER':'solar'}, {'IS_PUNCT':True}, {'LOWER':'power'}] \n",
    "pattern3 = [{'LOWER':'solar'},{'LOWER':'power'}]\n",
    "\n",
    "# to add a matcher with these patterns\n",
    "matcher.add('SolarPower', [pattern1, pattern2, pattern3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5631700",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The Solar Power industry is growing more with the solarpower increment. The team Solar-Power is good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd885c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 9, 10), (8656102463236116519, 14, 17)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd56f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match_ID               StringID   Starting_Idx   Ending_Idx   Text_span\n",
      "8656102463236116519    SolarPower     1              3         Solar Power\n",
      "8656102463236116519    SolarPower     9              10        solarpower\n",
      "8656102463236116519    SolarPower     14             17        Solar-Power\n"
     ]
    }
   ],
   "source": [
    "print(f\"Match_ID{'':15}StringID{'':3}Starting_Idx{'':3}Ending_Idx{'':3}Text_span\")\n",
    "for match_id, start, end in found_matches:\n",
    "    stringID = nlp.vocab.strings[match_id]  # to get string representation\n",
    "    span = doc[start:end]\n",
    "    print(f\"{match_id:<23}{stringID:<15}{start:<15}{end:<10}{span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4398beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove a pattern\n",
    "matcher.remove('SolarPower')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP GPU",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
