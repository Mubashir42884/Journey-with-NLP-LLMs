{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad623b5",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "For NLP tasks, we will be using Spacy library.\n",
    "\n",
    "For in-text Notebook installation:\n",
    "> `!pip install spacy`\n",
    "\n",
    "Using CMD  or Anaconda Prompt Command Prompt for installation:\n",
    "> `open cmd > Install with prompts \"conda install -c conda-forge spacy\" (RECOMMENDED!)`\n",
    "\n",
    "Download the language library (en_core_web_sm) for Spacy via CMD/Anaconda:\n",
    "> `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36e70b",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "NLP is and area of CS and AI that is concerned with the interactions between machine and human (natural) languages. Basically, programming the machines to process and analyze human natural language in bigger scale.\n",
    "\n",
    "In general, NLP processing looks like this:\n",
    "> ***Natural Human Language (speech and text in English, French, Bengali) <br> ‚Üí Raw Data (text, speech data) <br> ‚Üí NLP processing (tokenization, normalization, segmentation) <br> ‚Üí Feature Extraction <br> ‚Üí Machine recognizible vector representation (BoW, TF-IDF, Word Embeddings) <br> ‚Üí Machine analysis (ML-DL tasks) <br> ‚Üí NLP Applications (spam detection, chatbots, sentiment analysis etc.)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f31d4d",
   "metadata": {},
   "source": [
    "# NLP Processing Steps\n",
    "\n",
    "There are quite a few steps in the whole processing part of natural language data. But in a traditional best-case scene, we can actually divide it into 9 separate simple steps:\n",
    "\n",
    "#### Step-01: Text Cleaning\n",
    "Removing irrelevant or noisy elements from the text. For example:\n",
    "```\n",
    "\"Hey! üôÇ Check out this link ###https://abc.com!!!\"\n",
    ">> [\"Hey! Check out this link\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-02: Lowercasing\n",
    "Converting all text to lowercase letters for ensuring consisteny over all texts. For example:\n",
    "```\n",
    "\"Hello, Adam Gross! I'm Senat Brown.\"\n",
    ">> [\"hello, Adam Gross! i'm Senat  Brown.\"] (‚úî Preferable way)\n",
    ">> [\"hello, adam gross! i'm senat  brown.\"] (‚ùå Not Preferable as it looses Named Entity Recognition)\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-03: Sentence Segmentation\n",
    "Splitting different lines and sentences into separate entity. For example:\n",
    "```\n",
    "\"Sally is mumbling. She might be nervous of speaking.\"\n",
    ">> [\"Sally is mumbling.\", \"She might be nervous of speaking.\"]\n",
    "```\n",
    "‚òÖ Useful for parsing and document-level analysis\n",
    "<br><br>\n",
    "\n",
    "#### Step-04: Tokenization\n",
    "Breaking sentences into preferable peices (Tokens) of words and symbols. For example:\n",
    "```\n",
    "\"I'm Gary Hunson. A DYI shop owner at Brisbey.\"\n",
    ">> [\"I'm\", \"Gary\", \"Hunson\", \".\", \"A\", \"DYI\", \"shop\", \"owner\", \"at\", \"Brisbey\", \".\"]\n",
    "```\n",
    "‚òÖ Tokenization depends on the task in-hand and the context we're working with. It will be clear in upcoming notebooks.\n",
    "<br><br>\n",
    "\n",
    "#### Step-05: Normalization\n",
    "Standardizing text formats to ensure consistency over all type of texts. Oftenly results in better accuracy in classification related tasks. This includes:\n",
    "- Expanding Contraction (don't ‚Üí do not)\n",
    "- Removing extra Punctuation (Hello!! ‚Üí Hello!)\n",
    "- Converting Numbers to text [optional] (3 ‚Üí three)\n",
    "\n",
    "For example:\n",
    "```\n",
    "\"I can't do 9 to 5 anymore!!!\"\n",
    ">> [\"I cannot do nine to five anymore!\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-06: Stopword Removal\n",
    "It is the process of eliminating very common words (am, was, is, to, a, an) that carry less standalone meaning in a sentence and often do not help a model distinguish between texts. For example:\n",
    "```\n",
    "\"I am learning representation learning techniques.\"\n",
    ">> [\"learning\", \"representation\", \"learning\", \"techniques\"]\n",
    "```\n",
    "‚òÖ Removing stopwords:\n",
    "- Reduces noise in text\n",
    "- Reduces vocabulary size\n",
    "- Improves model efficieny\n",
    "- Focuses on content-bearing words\n",
    "<br>\n",
    "\n",
    "#### Step-07: Stemming or Lemmatization\n",
    "**Stemming** is not always a good choice, but faster processing technique.\n",
    "> `studying ‚Üí studi` | Faster processing but not a proper stem<br>\n",
    "> `running ‚Üí run` | A proper stem\n",
    "\n",
    "On the other hand, **Lemmatization** is preferable and accurate in practice, but slower in processing.\n",
    "> `studying ‚Üí study` | A proper lemma <br>\n",
    "> `running ‚Üí run` | A proper lemma\n",
    "<br>\n",
    "\n",
    "#### Step-08: Handling Rare Words and Noise\n",
    "Removing very unknown words and replaceing it with `<UNK>` token is often used for better processing. For example:\n",
    "\n",
    "```\n",
    "\"qwerty keyboard\"\n",
    ">> [<UNK>, \"keyboard\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Step-09: Vectorization\n",
    "This is a crucial and final process of converting the tokenized texts into numerical vector representation that machine can understand. This is done by various techniques, like:\n",
    "1. **Bag of Words (BoW)** = Represents text by counting the number of times each word appears and ignores the word order.\n",
    "```\n",
    "Text = \"learning representation learning techniques\"\n",
    "Tokens = [\"learning\", \"representation\", \"techniques\"]\n",
    "Vector = [2, 1, 1] (learning 2x, representation 1x, techniques 1x)\n",
    "```\n",
    "2. **TF-IDF (Term Frequency‚ÄìInverse Document Frequency)** = It improves BoW by reducing the importance of very common words (is, was, am) and highlighting distinctive words.<br>\n",
    "TF: how often a word appears in a document<br>\n",
    "IDF: how rare the word is across documents\n",
    "```\n",
    "learning ‚Üí 0.32,        representation ‚Üí 0.78,        techniques ‚Üí 0.64\n",
    "Vector = [0.32, 0.78, 0.64]\n",
    "```\n",
    "3. **Word Embeddings** = Word embeddings represent each word as a dense numerical vector that captures semantic meaning. Words with similar meanings have similar vectors.<br>\n",
    "```\n",
    "learning        ‚Üí [0.21, -0.34, 0.87, ...]\n",
    "representation  ‚Üí [0.19, -0.30, 0.82, ...]\n",
    "techniques     ‚Üí [0.25, -0.40, 0.90, ...]\n",
    "Key Learning:\n",
    "representation learning ‚âà feature learning (the word \"feature\" would have quite similar vector like \"representation\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92821d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP GPU",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
